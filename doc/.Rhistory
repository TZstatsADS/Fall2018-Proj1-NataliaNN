docs <- tm_map(docs, removeWords, c("happy", "got", "went", "made", "day", "time", "just", "last", "great", "get"))
# creating term document matrix
tdm <- TermDocumentMatrix(docs)
# defining tdm as matrix
m <- as.matrix(tdm)
# getting word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
# creating a data frame with words and their frequencies
text_wc_df <- data.frame(word=names(word_freqs), freq=word_freqs)
text_wc_df <- text_wc_df[1:300,]
# plotting wordcloud
set.seed(1234)
wordcloud(words = text_wc_df$word, freq = text_wc_df$freq,
min.freq = 1,scale=c(5,.3),
max.words=1, random.order=FALSE, rot.per=0.15,
colors=brewer.pal(8, "Dark2"))
happy_text <- newdata$cleaned_hm
docs <- Corpus(VectorSource(happy_text))
# Converting the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Removing english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removeWords, c("happy", "got", "went", "made", "day", "time", "just", "last", "great", "get"))
# creating term document matrix
tdm <- TermDocumentMatrix(docs)
# defining tdm as matrix
m <- as.matrix(tdm)
# getting word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
# creating a data frame with words and their frequencies
text_wc_df <- data.frame(word=names(word_freqs), freq=word_freqs)
text_wc_df <- text_wc_df[1:300,]
# plotting wordcloud
set.seed(1234)
wordcloud(words = text_wc_df$word, freq = text_wc_df$freq,
min.freq = 1,scale=c(5,.3),
max.words=500, random.order=FALSE, rot.per=0.15,
colors=brewer.pal(8, "Dark2"))
happy_text <- newdata$cleaned_hm
docs <- Corpus(VectorSource(happy_text))
# Converting the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Removing english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removeWords, c("happy", "got", "went", "made", "day", "time", "just", "last", "great", "get"))
# creating term document matrix
tdm <- TermDocumentMatrix(docs)
# defining tdm as matrix
m <- as.matrix(tdm)
# getting word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
# creating a data frame with words and their frequencies
text_wc_df <- data.frame(word=names(word_freqs), freq=word_freqs)
text_wc_df <- text_wc_df[1:300,]
# plotting wordcloud
set.seed(1234)
wordcloud(words = text_wc_df$word, freq = text_wc_df$freq,
min.freq = 1,scale=c(5,.3),
max.words=1000, random.order=FALSE, rot.per=0.15,
colors=brewer.pal(8, "Dark2"))
happy_text <- newdata$cleaned_hm
docs <- Corpus(VectorSource(happy_text))
# Converting the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Removing english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removeWords, c("happy", "got", "went", "made", "day", "time", "just", "last", "great", "get"))
# creating term document matrix
tdm <- TermDocumentMatrix(docs)
# defining tdm as matrix
m <- as.matrix(tdm)
# getting word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
# creating a data frame with words and their frequencies
text_wc_df <- data.frame(word=names(word_freqs), freq=word_freqs)
text_wc_df <- text_wc_df[1:2,]
# plotting wordcloud
set.seed(1234)
wordcloud(words = text_wc_df$word, freq = text_wc_df$freq,
min.freq = 1,scale=c(5,.3),
max.words=1000, random.order=FALSE, rot.per=0.15,
colors=brewer.pal(8, "Dark2"))
happy_text <- newdata$cleaned_hm
docs <- Corpus(VectorSource(happy_text))
# Converting the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Removing english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removeWords, c("happy", "got", "went", "made", "day", "time", "just", "last", "great", "get"))
# creating term document matrix
tdm <- TermDocumentMatrix(docs)
# defining tdm as matrix
m <- as.matrix(tdm)
# getting word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
# creating a data frame with words and their frequencies
text_wc_df <- data.frame(word=names(word_freqs), freq=word_freqs)
text_wc_df <- text_wc_df[1:500,]
# plotting wordcloud
set.seed(1234)
wordcloud(words = text_wc_df$word, freq = text_wc_df$freq,
min.freq = 1,scale=c(5,.3),
max.words=500, random.order=FALSE, rot.per=0.15,
colors=brewer.pal(8, "Dark2"))
happy_text <- newdata$cleaned_hm
docs <- Corpus(VectorSource(happy_text))
# Converting the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Removing english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removeWords, c("happy", "got", "went", "made", "day", "time", "just", "last", "great", "get"))
# creating term document matrix
tdm <- TermDocumentMatrix(docs)
# defining tdm as matrix
m <- as.matrix(tdm)
# getting word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
# creating a data frame with words and their frequencies
text_wc_df <- data.frame(word=names(word_freqs), freq=word_freqs)
text_wc_df <- text_wc_df[1:1000,]
# plotting wordcloud
set.seed(1234)
wordcloud(words = text_wc_df$word, freq = text_wc_df$freq,
min.freq = 1,scale=c(5,.5),
max.words=1000, random.order=FALSE, rot.per=0.15,
colors=brewer.pal(8, "Dark2"))
count_bigrams <- function(dataset) {
dataset %>%
unnest_tokens(bigram, cleaned_hm , token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word,
!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE)
}
text_bigrams <- newdata %>%
count_bigrams()
head(text_bigrams, 10)
install.packages("magrittr")
install.packages("magrittr")
install.packages("syuzhet")
install.packages("ggraph")
install.packages("igraph")
install.packages("tidyr")
install.packages("tidyr")
install.packages("readr")
install.packages("readr")
install.packages("circlize")
install.packages("reshape2")
library(tidyverse)
library(tidytext)
library(DT)
library(scales)
library(wordcloud)
library(gridExtra)
library(ngram)
library(shiny)
library(dplyr)
library(ggplot2)
library(tm)
library(magrittr)
library(stringr)
library(syuzhet)
library(tidyr)
library(igraph)
library(ggraph)
install.packages("ggraph")
library(tidyverse)
library(tidytext)
library(DT)
library(scales)
library(wordcloud)
library(gridExtra)
library(ngram)
library(shiny)
library(dplyr)
library(ggplot2)
library(tm)
library(magrittr)
library(stringr)
library(syuzhet)
library(tidyr)
library(igraph)
library(ggraph)
library(tidyverse)
library(tidytext)
library(DT)
library(scales)
library(wordcloud)
library(gridExtra)
library(ngram)
library(shiny)
library(dplyr)
library(ggplot2)
library(tm)
library(magrittr)
library(stringr)
library(syuzhet)
library(tidyr)
library(igraph)
# library(ggraph)
library(readr)
library(circlize)
library(reshape2)
visualize_bigrams <- function(bigrams) {
set.seed(2016)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
bigrams %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
ggtitle("Network graph of bigrams") +
theme_void()
}
textgr_bigrams %>%
filter(n > 3,
!str_detect(word1, "\\d"),
!str_detect(word2, "\\d")) %>%
visualize_bigrams()
visualize_bigrams <- function(bigrams) {
set.seed(2016)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
bigrams %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
ggtitle("Network graph of bigrams") +
theme_void()
}
text_bigrams %>%
filter(n > 3,
!str_detect(word1, "\\d"),
!str_detect(word2, "\\d")) %>%
visualize_bigrams()
library("ggraph")
library(ggraph)
happy_text <- newdata$cleaned_hm
docs <- Corpus(VectorSource(happy_text))
# Converting the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Removing english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removeWords, c("happy", "got", "went", "made", "day", "time", "just", "last", "great", "get"))
# creating term document matrix
tdm <- TermDocumentMatrix(docs)
# defining tdm as matrix
m <- as.matrix(tdm)
# getting word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
# creating a data frame with words and their frequencies
text_wc_df <- data.frame(word=names(word_freqs), freq=word_freqs)
text_wc_df <- text_wc_df[1:1000,]
# plotting wordcloud
set.seed(1234)
wordcloud(words = text_wc_df$word, freq = text_wc_df$freq,
min.freq = 1,scale=c(5,.5),
max.words=1000, random.order=FALSE, rot.per=0.15,
colors=brewer.pal(10, "Dark2"))
library(tidyverse)
library(tidytext)
library(DT)
library(scales)
library(wordcloud)
library(gridExtra)
library(ngram)
library(shiny)
library(dplyr)
library(ggplot2)
library(tm)
library(igraph)
library(ggraph)
library(text2vec)
library(data.table)
library(magrittr)
library(glmnet)
urlfile1 <- 'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/cleaned_hm.csv'
hm_data <- read_csv(urlfile1)
urlfile2 <- 'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demographic <- read_csv(urlfile2)
fulldata <- full_join(hm_data, demographic, c("wid" = "wid"))
new <- fulldata[,-1]
newdata <- new %>%
filter(gender %in% c("m", "f")) %>%
filter(marital %in% c("single", "married")) %>%
filter(parenthood %in% c("n", "y")) %>%
filter(reflection_period %in% c("24h", "3m")) %>%
mutate(reflection_period = fct_recode(reflection_period,
months_3 = "3m", hours_24 = "24h"))
newdata <- na.omit(newdata)
# head(newdata)
word.count <- vector(length = length(newdata$cleaned_hm))
for (i in 1:length(newdata$cleaned_hm)) {
word.count[i] <- wordcount(newdata$cleaned_hm[i], sep = " ", count.function = sum)
}
newnewdata <- cbind(newdata,word.count)
ggplot(newnewdata, aes(x=word.count)) +
xlim(0,100) +
geom_histogram(bins=30, aes(fill = ..count..)) +
geom_vline(aes(xintercept=mean(word.count)),
color="#FFFFFF", linetype="dashed", size=1) +
geom_density(aes(y=4 * ..count..),alpha=.2, fill="#1CCCC6") +
ylab("num of moments") + xlab ("word.numbers") +
ggtitle("Distribution of word count") +
theme_minimal()
happy_text <- newdata$cleaned_hm
docs <- Corpus(VectorSource(happy_text))
# Converting the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Removing english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removeWords, c("happy", "got", "went", "made", "day", "time", "just", "last", "great", "get"))
# creating term document matrix
tdm <- TermDocumentMatrix(docs)
# defining tdm as matrix
m <- as.matrix(tdm)
# getting word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
# creating a data frame with words and their frequencies
text_wc_df <- data.frame(word=names(word_freqs), freq=word_freqs)
text_wc_df <- text_wc_df[1:500,]
# plotting wordcloud
set.seed(1234)
wordcloud(words = text_wc_df$word, freq = text_wc_df$freq,
min.freq = 0,scale=c(5,.5),
max.words=300, random.order=FALSE, rot.per=0.15,
colors=brewer.pal(10, "Dark2"))
count_bigrams <- function(dataset) {
dataset %>%
unnest_tokens(bigram, cleaned_hm , token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word,
!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE)
}
text_bigrams <- newdata %>%
count_bigrams()
head(text_bigrams, 10)
visualize_bigrams <- function(bigrams) {
set.seed(2018)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
bigrams %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
ggtitle("Network graph of bigrams") +
theme_void()
}
text_bigrams %>%
filter(n > 14,
!str_detect(word1, "\\d"),
!str_detect(word2, "\\d")) %>%
visualize_bigrams()
ggplot(data.frame(prop.table(table(newdata$predicted_category))), aes(x=Var1, y = Freq*100, fill = Var1)) +
geom_bar(stat = 'identity') +
xlab('Happy Category') +
ylab('Percentage of Happy Moments (%)') +
geom_text(aes(label=round(Freq*100,2)), vjust=-0.25) +
ggtitle('Percentage of Happy Moments by Category')
newdata$age <- as.numeric(as.character(newdata$age))
ages <- newdata  %>% select(wid, age, predicted_category) %>% mutate(Age_group = ifelse(age < 18, '0-18',ifelse(age < 30, '18-29', ifelse(age < 40, '30-39', ifelse(age < 50, '40-49', ifelse(age < 60, '50-59', ifelse(age < 70, '60-69', ifelse(age < 80, '70-79', ifelse(age < 90, '80-89', '90-99')))))))))
ages <- ages %>% mutate(Age_group = factor(Age_group), predicted_category = factor(predicted_category, levels = rev(c( 'achievement', 'affection', 'bonding',  'enjoy_the_moment','exercise', 'leisure', 'nature'))))
ages %>% filter(age < 100) %>% group_by(Age_group) %>% count(predicted_category) %>% ggplot(aes(predicted_category, n, fill = Age_group)) + geom_bar(stat='identity', show.legend = FALSE) + facet_wrap(~Age_group, scales = 'free') + xlab('Happy Category') + ylab('Number of Happy Moments') + geom_text(aes(label = n), hjust = .73) + coord_flip()
genders <- newdata  %>% select(wid, gender, predicted_category) %>% mutate(Gender_group = ifelse(gender == "f" , 'female',ifelse(gender == "m", 'male',NA)))
genders <- genders %>% mutate(Gender_group = factor(Gender_group), predicted_category = factor(predicted_category, levels = rev(c( 'achievement', 'affection', 'bonding',  'enjoy_the_moment','exercise', 'leisure', 'nature'))))
genders %>% group_by(Gender_group) %>% count(predicted_category) %>% ggplot(aes(predicted_category, n, fill = Gender_group)) + geom_bar(stat='identity', show.legend = FALSE) + facet_wrap(~Gender_group, scales = 'free') + xlab('Happy Category') + ylab('Number of Happy Moments') + geom_text(aes(label = n), hjust = .73) + coord_flip()
Marital <- newdata  %>% select(wid, marital, predicted_category) %>% mutate(Marital_group = ifelse(marital == "married" , 'married',ifelse(marital == "single", 'single',NA)))
Marital <- Marital %>% mutate(Marital_group = factor(Marital_group), predicted_category = factor(predicted_category, levels = rev(c( 'achievement', 'affection', 'bonding',  'enjoy_the_moment','exercise', 'leisure', 'nature'))))
Marital %>% group_by(Marital_group) %>% count(predicted_category) %>% ggplot(aes(predicted_category, n, fill = Marital_group)) + geom_bar(stat='identity', show.legend = FALSE) + facet_wrap(~Marital_group, scales = 'free') + xlab('Happy Category') + ylab('Number of Happy Moments') + geom_text(aes(label = n), hjust = .73) + coord_flip()
genderdata <- cbind(newdata[,4], newdata[,11])
genderdata$dum <- ifelse(genderdata$gender == "f", 1, 0)
head(genderdata$cleaned_hm)
trainnum <- round(0.7*nrow(genderdata))
set.seed(2018)
traingender <- genderdata[sample(c(1:nrow(genderdata)), size = trainnum),]
testgender <- genderdata[-sample(c(1:nrow(genderdata)), size = trainnum),]
## Vocabulary-based vectorization
prep_fun <- tolower
tok_fun <- word_tokenizer
it_train <- itoken(traingender$cleaned_hm,
preprocessor = prep_fun,
tokenizer = tok_fun,
genders = traingender$dum,
progressbar = FALSE)
vocab <- create_vocabulary(it_train)
train_tokens <- traingender$cleaned_hm %>%
prep_fun %>%
tok_fun
it_train <- itoken(train_tokens,
genders = traingender$dum,
# turn off progressbar because it won't look nice in rmd
progressbar = FALSE)
vocab <- create_vocabulary(it_train)
vectorizer <- vocab_vectorizer(vocab)
t1 = Sys.time()
dtm_train = create_dtm(it_train, vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))
# Logistic Regression
NFOLDS = 4
t1 = Sys.time()
glmnet_classifier = cv.glmnet(x = dtm_train, y = traingender$dum,
family = 'binomial',
alpha = 1, # L1 penalty
type.measure = "auc",# the area under ROC curve
nfolds = NFOLDS,# 5-fold cross-validation
thresh = 1e-3,  # high value is less accurate, but has faster training
maxit = 1e3)# lower number of iterations for faster training
print(difftime(Sys.time(), t1, units = 'sec'))
plot(glmnet_classifier)
print(paste("max AUC =", round(max(glmnet_classifier$cvm), 4)))
# Test the Classifier
it_test = testgender$cleaned_hm %>%
prep_fun %>% tok_fun %>%
# turn off progressbar because it won't look nice in rmd
itoken(genders = testgender$dum, progressbar = FALSE)
dtm_test = create_dtm(it_test, vectorizer)
preds = predict(glmnet_classifier, dtm_test, type = 'response')[,1]
glmnet:::auc(testgender$dum, preds)
maritaldata <- cbind(newdata[,4], newdata[,12])
maritaldata$dum <- ifelse(maritaldata$marital== "married", 1, 0)
trainnum <- round(0.7*nrow(maritaldata))
set.seed(2018)
trainmarital <- maritaldata[sample(c(1:nrow(maritaldata)), size = trainnum),]
testmarital <- maritaldata[-sample(c(1:nrow(maritaldata)), size = trainnum),]
prep_funm <- tolower
tok_funm <- word_tokenizer
it_trainm <- itoken(trainmarital$cleaned_hm,
preprocessor = prep_funm,
tokenizer = tok_funm,
maritals = trainmarital$dum,
progressbar = FALSE)
vocabm <- create_vocabulary(it_trainm)
train_tokensm<- trainmarital$cleaned_hm %>%
prep_funm %>%
tok_funm
it_trainm <- itoken(train_tokensm,
maritals = trainmarital$dum,
progressbar = FALSE)
vocabm <- create_vocabulary(it_trainm)
vectorizerm <- vocab_vectorizer(vocabm)
t1m = Sys.time()
dtm_trainm = create_dtm(it_trainm, vectorizerm)
print(difftime(Sys.time(), t1m, units = 'sec'))
# Logistic Regression
NFOLDS = 4
t1m = Sys.time()
glmnet_classifierm = cv.glmnet(x = dtm_trainm, y = trainmarital$dum,
family = 'binomial',
alpha = 1, # L1 penalty
type.measure = "auc",# the area under ROC curve
nfolds = NFOLDS,# 5-fold cross-validation
thresh = 1e-3,  # high value is less accurate, but has faster training
maxit = 1e3)# lower number of iterations for faster training
print(difftime(Sys.time(), t1m, units = 'sec'))
plot(glmnet_classifierm)
print(paste("max AUC =", round(max(glmnet_classifierm$cvm), 4)))
# Test the Classifier
it_testm = testmarital$cleaned_hm %>%
prep_funm %>% tok_funm %>%
itoken(maritals = testmarital$dum, progressbar = FALSE)
dtm_testm = create_dtm(it_testm, vectorizerm)
predsm = predict(glmnet_classifierm, dtm_testm, type = 'response')[,1]
glmnet:::auc(testmarital$dum, predsm)
parentdata <- cbind(newdata[,4], newdata[,13])
parentdata$dum <- ifelse(parentdata$parenthood== "y", 1, 0)
trainnum <- round(0.7*nrow(parentdata))
set.seed(2018)
trainparent <- parentdata[sample(c(1:nrow(parentdata)), size = trainnum),]
testparent <- parentdata[-sample(c(1:nrow(parentdata)), size = trainnum),]
prep_funp <- tolower
tok_funp <- word_tokenizer
it_trainp <- itoken(trainparent$cleaned_hm,
preprocessor = prep_funp,
tokenizer = tok_funp,
parents = trainparent$dum,
progressbar = FALSE)
vocabp <- create_vocabulary(it_trainp)
train_tokensp <- trainparent$cleaned_hm %>%
prep_funp %>%
tok_funp
it_trainp <- itoken(train_tokensp,
parents = trainparent$dum,
progressbar = FALSE)
vocabp <- create_vocabulary(it_trainp)
vectorizerp <- vocab_vectorizer(vocabp)
t1p = Sys.time()
dtm_trainp = create_dtm(it_trainp, vectorizerp)
print(difftime(Sys.time(), t1p, units = 'sec'))
# Logistic Regression
NFOLDS = 4
t1p = Sys.time()
glmnet_classifierp = cv.glmnet(x = dtm_trainp, y = trainparent$dum,
family = 'binomial',
alpha = 1, # L1 penalty
type.measure = "auc",# the area under ROC curve
nfolds = NFOLDS,# 5-fold cross-validation
thresh = 1e-3,  # high value is less accurate, but has faster training
maxit = 1e3)# lower number of iterations for faster training
print(difftime(Sys.time(), t1p, units = 'sec'))
plot(glmnet_classifierp)
print(paste("max AUC =", round(max(glmnet_classifierp$cvm), 4)))
# Test the Classifier
it_testp = testparent$cleaned_hm %>%
prep_funp %>% tok_funp %>%
itoken(parents = testparent$dum, progressbar = FALSE)
dtm_testp = create_dtm(it_testp, vectorizerp)
predsp = predict(glmnet_classifierp, dtm_testp, type = 'response')[,1]
glmnet:::auc(testparent$dum, predsp)
