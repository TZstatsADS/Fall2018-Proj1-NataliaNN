---
title: "project1"
author: "Nannan Wang"
date: "September 17, 2018"
output:
  html_document: default
  pdf_document: default
---
```{r, message= F,warning = FALSE}
library(tidyverse)
library(tidytext)
library(DT)
library(scales)
library(wordcloud)
library(gridExtra)
library(ngram)
library(shiny)
library(dplyr)
library(ggplot2)
library(tm)
library(igraph)
library(ggraph)
library(text2vec)
library(data.table)
library(magrittr)
library(glmnet)
library(topicmodels)
```

## 1.Data Preparation 
```{r, message= F,warning = FALSE}
urlfile1 <- 'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/cleaned_hm.csv'
hm_data <- read_csv(urlfile1)
urlfile2 <- 'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'
demographic <- read_csv(urlfile2)
fulldata <- full_join(hm_data, demographic, c("wid" = "wid"))
new <- fulldata[,-1]
newdata <- new %>%
  filter(gender %in% c("m", "f")) %>%
  filter(marital %in% c("single", "married")) %>%
  filter(parenthood %in% c("n", "y")) %>%
  filter(reflection_period %in% c("24h", "3m")) %>%
  mutate(reflection_period = fct_recode(reflection_period, 
                                        months_3 = "3m", hours_24 = "24h"))
newdata <- na.omit(newdata)
# head(newdata)
```

## 2.Data Presentation
In this part, we use different forms to display the happy moments' text and explore some interesting details.

###2.1 Word Count
```{r,message= F,warning = FALSE}
word.count <- vector(length = length(newdata$cleaned_hm))
for (i in 1:length(newdata$cleaned_hm)) {
  word.count[i] <- wordcount(newdata$cleaned_hm[i], sep = " ", count.function = sum)
}
newnewdata <- cbind(newdata,word.count)
ggplot(newnewdata, aes(x=word.count)) +
  xlim(0,100) +
  geom_histogram(bins=30, aes(fill = ..count..)) + 
  geom_vline(aes(xintercept=mean(word.count)),
             color="#FFFFFF", linetype="dashed", size=1) +
  geom_density(aes(y=4 * ..count..),alpha=.2, fill="#1CCCC6") +
  ylab("num of moments") + xlab ("word.numbers") +
  ggtitle("Distribution of word count") +
  theme_minimal()
```
In this Part, I count the word number for each happy moment discribtion, and most people can express their happiness with less than 15 words. It perhaps shows that happiness do not need too much words to speak out.

###2.2 Word Frequency
```{r, message= F,warning = FALSE}
happy_text <- newdata$cleaned_hm
docs <- Corpus(VectorSource(happy_text))
# Converting the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Removing english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removeWords, c("happy", "got", "went", "made", "day", "time", "just", "last", "great", "get"))
# creating term document matrix
tdm <- TermDocumentMatrix(docs)
# defining tdm as matrix
m <- as.matrix(tdm)
# getting word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing=TRUE)
# creating a data frame with words and their frequencies
text_wc_df <- data.frame(word=names(word_freqs), freq=word_freqs)
text_wc_df <- text_wc_df[1:500,]
# plotting wordcloud
set.seed(1234)
wordcloud(words = text_wc_df$word, freq = text_wc_df$freq,
min.freq = 0,scale=c(5,.5),
max.words=300, random.order=FALSE, rot.per=0.15,
colors=brewer.pal(10, "Dark2"))
```

In this part, We find that some words appear most frenquently in people's happy moments, such like: "work", "friend", "new", "family", "son","game", "birthday" etc.

###2.3 Bigrams 
```{r, message= F,warning = FALSE}
count_bigrams <- function(dataset) {
dataset %>%
unnest_tokens(bigram, cleaned_hm , token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word,
!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE)
}
 
text_bigrams <- newdata %>%
count_bigrams()
 
head(text_bigrams, 10)
```

In this part, we focus on the bigrams which are phrases we used in the daily life. In terms of top 10 bigrams, we find top three meaningful phrases which play very important roles in people's happy moments:1)birthday party 2)video game 3)ice cream, that is amazing!

###2.4 Bigrams Visualization
```{r,message= F,warning = FALSE}
visualize_bigrams <- function(bigrams) {
set.seed(2018)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
 
bigrams %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
ggtitle("Network graph of bigrams") +
theme_void()
}
 
text_bigrams %>%
 filter(n > 14,
       !str_detect(word1, "\\d"),
       !str_detect(word2, "\\d")) %>%
visualize_bigrams()
```

In this part, based on the bigrams we focused, we visualize the bigrams using the network graph, and we can find the most popular happy moments and their connections! For instance, the wedding/date/marriage can be connected together.

## 3.Exploration of Happy Moments
In this part, we wan to explore different Categories of Happy Moments, and we also explore the moments by different age groups/ gender groups/ marital groups.
###3.1 Percentage of Happy Moments by Category
```{r,message= F,warning = FALSE}
ggplot(data.frame(prop.table(table(newdata$predicted_category))), aes(x=Var1, y = Freq*100, fill = Var1)) + 
  geom_bar(stat = 'identity') + 
  xlab('Happy Category') + 
  ylab('Percentage of Happy Moments (%)') + 
  geom_text(aes(label=round(Freq*100,2)), vjust=-0.25) + 
  ggtitle('Percentage of Happy Moments by Category')
```

In this part, we find that based on all the happy moments, people get their happiness from affenction most, and then the achivement, which is related to the wordcloud we generated above.

###3.2 Happy Category by Age
```{r,message= F,warning = FALSE}
newdata$age <- as.numeric(as.character(newdata$age))
ages <- newdata  %>% select(wid, age, predicted_category) %>% mutate(Age_group = ifelse(age < 18, '0-18',ifelse(age < 30, '18-29', ifelse(age < 40, '30-39', ifelse(age < 50, '40-49', ifelse(age < 60, '50-59', ifelse(age < 70, '60-69', ifelse(age < 80, '70-79', ifelse(age < 90, '80-89', '90-99')))))))))

ages <- ages %>% mutate(Age_group = factor(Age_group), predicted_category = factor(predicted_category, levels = rev(c( 'achievement', 'affection', 'bonding',  'enjoy_the_moment','exercise', 'leisure', 'nature'))))

ages %>% filter(age < 100) %>% group_by(Age_group) %>% count(predicted_category) %>% ggplot(aes(predicted_category, n, fill = Age_group)) + geom_bar(stat='identity', show.legend = FALSE) + facet_wrap(~Age_group, scales = 'free') + xlab('Happy Category') + ylab('Number of Happy Moments') + geom_text(aes(label = n), hjust = .73) + coord_flip()
```

In this part, based on different age group, we can not explore significant difference bewteen different groups, they all get happiness from affection most, and second is the achievement.

###3.3 Happy Category by Gender
```{r,message= F,warning = FALSE}
genders <- newdata  %>% select(wid, gender, predicted_category) %>% mutate(Gender_group = ifelse(gender == "f" , 'female',ifelse(gender == "m", 'male',NA)))

genders <- genders %>% mutate(Gender_group = factor(Gender_group), predicted_category = factor(predicted_category, levels = rev(c( 'achievement', 'affection', 'bonding',  'enjoy_the_moment','exercise', 'leisure', 'nature'))))

genders %>% group_by(Gender_group) %>% count(predicted_category) %>% ggplot(aes(predicted_category, n, fill = Gender_group)) + geom_bar(stat='identity', show.legend = FALSE) + facet_wrap(~Gender_group, scales = 'free') + xlab('Happy Category') + ylab('Number of Happy Moments') + geom_text(aes(label = n), hjust = .73) + coord_flip()
```

In this part, we find that male get happiness from achievement most which is totally different from the results above, and also exercise accounts for a lot by male than female. That makes sense!

###3.4 Happy Category by Marital Status
```{r,message= F,warning = FALSE}
Marital <- newdata  %>% select(wid, marital, predicted_category) %>% mutate(Marital_group = ifelse(marital == "married" , 'married',ifelse(marital == "single", 'single',NA)))

Marital <- Marital %>% mutate(Marital_group = factor(Marital_group), predicted_category = factor(predicted_category, levels = rev(c( 'achievement', 'affection', 'bonding',  'enjoy_the_moment','exercise', 'leisure', 'nature'))))

Marital %>% group_by(Marital_group) %>% count(predicted_category) %>% ggplot(aes(predicted_category, n, fill = Marital_group)) + geom_bar(stat='identity', show.legend = FALSE) + facet_wrap(~Marital_group, scales = 'free') + xlab('Happy Category') + ylab('Number of Happy Moments') + geom_text(aes(label = n), hjust = .73) + coord_flip()
```

In this part, we find that single people get happiness from achievement most and the affection is the second, which perhaphs means that single people have less happiness from affection whithout their own kids and husband(wife).

## LDA 
```{r}
#Find the sum of words in each Document
#rowTotals <- apply(tdm , 1, sum)
#tdm <- tdm[rowTotals > 0, ]
#topic <- LDA(text_wc_df, k=5, method = "Gibbs", 
            #control = list(seed = 2018,
                           # burnin = 1000,
                           # thin = 100,
                           # iter = 1000,
                           # alpha = 0.5))
#terms(topic,5)
```

## 4. Logistic Regrssion
In this part, we want to explore deeper in people's happy moments, we can apply logistic regression to build some classifiers to recognize people's gender/marital status/parenthood status according to their happy moment descriptions.
###4.1 Classifier for Gender
```{r,message= F,warning = FALSE}
genderdata <- cbind(newdata[,4], newdata[,11])
genderdata$dum <- ifelse(genderdata$gender == "f", 1, 0)
head(genderdata$cleaned_hm)
trainnum <- round(0.7*nrow(genderdata))
set.seed(2018)
traingender <- genderdata[sample(c(1:nrow(genderdata)), size = trainnum),]
testgender <- genderdata[-sample(c(1:nrow(genderdata)), size = trainnum),]
## Vocabulary-based vectorization
prep_fun <- tolower
tok_fun <- word_tokenizer

it_train <- itoken(traingender$cleaned_hm, 
             preprocessor = prep_fun, 
             tokenizer = tok_fun, 
             genders = traingender$dum, 
             progressbar = FALSE)
vocab <- create_vocabulary(it_train)
train_tokens <- traingender$cleaned_hm %>% 
  prep_fun %>% 
  tok_fun
it_train <- itoken(train_tokens, 
                  genders = traingender$dum,
                  # turn off progressbar because it won't look nice in rmd
                  progressbar = FALSE)

vocab <- create_vocabulary(it_train)
vectorizer <- vocab_vectorizer(vocab)
t1 = Sys.time()
dtm_train = create_dtm(it_train, vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))

# Logistic Regression
NFOLDS = 4
t1 = Sys.time()
glmnet_classifier = cv.glmnet(x = dtm_train, y = traingender$dum, 
                              family = 'binomial', 
                              alpha = 1, # L1 penalty
                              type.measure = "auc",# the area under ROC curve
                              nfolds = NFOLDS,# 5-fold cross-validation
                              thresh = 1e-3,  # high value is less accurate, but has faster training
                              maxit = 1e3)# lower number of iterations for faster training
print(difftime(Sys.time(), t1, units = 'sec'))
plot(glmnet_classifier)
print(paste("max AUC =", round(max(glmnet_classifier$cvm), 4)))

# Test the Classifier
it_test = testgender$cleaned_hm %>% 
  prep_fun %>% tok_fun %>% 
  # turn off progressbar because it won't look nice in rmd
  itoken(genders = testgender$dum, progressbar = FALSE)
         

dtm_test = create_dtm(it_test, vectorizer)

preds = predict(glmnet_classifier, dtm_test, type = 'response')[,1]
glmnet:::auc(testgender$dum, preds)
```
In this part, we make a classifier to classify people's gender. First, split 70% data randomly as train data and the rest 30% are test data. And then apply the logistic regression for train data. At last test the calssifier on the test data. In this case, we find the the classifier accurate is about 0.7297626 tested on the test data, the classifier works pretty well.

###4.2 Classifier for Marital Status
```{r,message= F,warning = FALSE}
maritaldata <- cbind(newdata[,4], newdata[,12])
maritaldata$dum <- ifelse(maritaldata$marital== "married", 1, 0)
trainnum <- round(0.7*nrow(maritaldata))
set.seed(2018)
trainmarital <- maritaldata[sample(c(1:nrow(maritaldata)), size = trainnum),]
testmarital <- maritaldata[-sample(c(1:nrow(maritaldata)), size = trainnum),]
prep_funm <- tolower
tok_funm <- word_tokenizer

it_trainm <- itoken(trainmarital$cleaned_hm, 
             preprocessor = prep_funm, 
             tokenizer = tok_funm, 
             maritals = trainmarital$dum, 
             progressbar = FALSE)
vocabm <- create_vocabulary(it_trainm)
train_tokensm<- trainmarital$cleaned_hm %>% 
  prep_funm %>% 
  tok_funm
it_trainm <- itoken(train_tokensm, 
                  maritals = trainmarital$dum,
                  progressbar = FALSE)

vocabm <- create_vocabulary(it_trainm)
vectorizerm <- vocab_vectorizer(vocabm)
t1m = Sys.time()
dtm_trainm = create_dtm(it_trainm, vectorizerm)
print(difftime(Sys.time(), t1m, units = 'sec'))
# Logistic Regression
NFOLDS = 4
t1m = Sys.time()
glmnet_classifierm = cv.glmnet(x = dtm_trainm, y = trainmarital$dum, 
                              family = 'binomial', 
                              alpha = 1, # L1 penalty
                              type.measure = "auc",# the area under ROC curve
                              nfolds = NFOLDS,# 5-fold cross-validation
                              thresh = 1e-3,  # high value is less accurate, but has faster training
                              maxit = 1e3)# lower number of iterations for faster training
print(difftime(Sys.time(), t1m, units = 'sec'))
plot(glmnet_classifierm)
print(paste("max AUC =", round(max(glmnet_classifierm$cvm), 4)))
# Test the Classifier
it_testm = testmarital$cleaned_hm %>% 
  prep_funm %>% tok_funm %>% 
  itoken(maritals = testmarital$dum, progressbar = FALSE)
         
dtm_testm = create_dtm(it_testm, vectorizerm)

predsm = predict(glmnet_classifierm, dtm_testm, type = 'response')[,1]
glmnet:::auc(testmarital$dum, predsm)
```
The same method as 4.1, and the accurate of marital classifier is 0.7783142, it also works well. The graph shows the max AUC is 0.7209.

###4.3 Classifier for Parenthood
```{r, message= F,warning = FALSE}
parentdata <- cbind(newdata[,4], newdata[,13])
parentdata$dum <- ifelse(parentdata$parenthood== "y", 1, 0)
trainnum <- round(0.7*nrow(parentdata))
set.seed(2018)
trainparent <- parentdata[sample(c(1:nrow(parentdata)), size = trainnum),]
testparent <- parentdata[-sample(c(1:nrow(parentdata)), size = trainnum),]
prep_funp <- tolower
tok_funp <- word_tokenizer

it_trainp <- itoken(trainparent$cleaned_hm, 
             preprocessor = prep_funp, 
             tokenizer = tok_funp, 
             parents = trainparent$dum, 
             progressbar = FALSE)
vocabp <- create_vocabulary(it_trainp)
train_tokensp <- trainparent$cleaned_hm %>% 
  prep_funp %>% 
  tok_funp
it_trainp <- itoken(train_tokensp, 
                  parents = trainparent$dum,
                  progressbar = FALSE)

vocabp <- create_vocabulary(it_trainp)
vectorizerp <- vocab_vectorizer(vocabp)
t1p = Sys.time()
dtm_trainp = create_dtm(it_trainp, vectorizerp)
print(difftime(Sys.time(), t1p, units = 'sec'))
# Logistic Regression
NFOLDS = 4
t1p = Sys.time()
glmnet_classifierp = cv.glmnet(x = dtm_trainp, y = trainparent$dum, 
                              family = 'binomial', 
                              alpha = 1, # L1 penalty
                              type.measure = "auc",# the area under ROC curve
                              nfolds = NFOLDS,# 5-fold cross-validation
                              thresh = 1e-3,  # high value is less accurate, but has faster training
                              maxit = 1e3)# lower number of iterations for faster training
print(difftime(Sys.time(), t1p, units = 'sec'))
plot(glmnet_classifierp)
print(paste("max AUC =", round(max(glmnet_classifierp$cvm), 4)))
# Test the Classifier
it_testp = testparent$cleaned_hm %>% 
  prep_funp %>% tok_funp %>% 
  itoken(parents = testparent$dum, progressbar = FALSE)
         
dtm_testp = create_dtm(it_testp, vectorizerp)

predsp = predict(glmnet_classifierp, dtm_testp, type = 'response')[,1]
glmnet:::auc(testparent$dum, predsp)
```
The same method as 4.1, and the accurate of parent classifier is 0.7241155, it also works well.
